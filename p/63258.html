<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ResNetV1,">





  <link rel="alternate" href="/atom.xml" title="孔明の博客" type="application/atom+xml">






<meta name="description" content="论文解决的主要问题是深层的神经网络很难训练；提出了一种残差学习框架来减轻网络训练。">
<meta name="keywords" content="ResNetV1">
<meta property="og:type" content="article">
<meta property="og:title" content="【图像分类—ResNet V1】Deep Residual Learning for Image Recognition">
<meta property="og:url" content="https://gkm0120.github.io/p/63258.html">
<meta property="og:site_name" content="孔明の博客">
<meta property="og:description" content="论文解决的主要问题是深层的神经网络很难训练；提出了一种残差学习框架来减轻网络训练。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021022417083119.png?text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTgzOTAzOQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117163439557.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117163620107.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117163809689.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117163857859.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164024281.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164108856.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2021011716423757.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164339696.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164446577.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164730510.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164654928.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201218141750496.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117164911125.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117165039510.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117165203540.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210117165237461.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210119103110772.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210119103800709.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210119104141236.png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210119105634292.png#pic_center">
<meta property="og:updated_time" content="2021-03-31T01:36:36.690Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【图像分类—ResNet V1】Deep Residual Learning for Image Recognition">
<meta name="twitter:description" content="论文解决的主要问题是深层的神经网络很难训练；提出了一种残差学习框架来减轻网络训练。">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2021022417083119.png?text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTgzOTAzOQ==,size_16,color_FFFFFF,t_70">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://gkm0120.github.io/p/63258.html">





  <title>【图像分类—ResNet V1】Deep Residual Learning for Image Recognition | 孔明の博客</title>
  








<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">孔明の博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://gkm0120.github.io/p/63258.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="戈孔明">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="孔明の博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">【图像分类—ResNet V1】Deep Residual Learning for Image Recognition</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-01-18T22:32:38+08:00">
                2021-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文翻译/" itemprop="url" rel="index">
                    <span itemprop="name">论文翻译</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/p/63258.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/p/63258.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-eye"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  11k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  39 min
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>论文解决的主要问题是深层的神经网络很难训练；提出了一种残差学习框架来减轻网络训练。</p>
<a id="more"></a>


<p><img src="https://img-blog.csdnimg.cn/2021022417083119.png?text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTgzOTAzOQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="一-、论文翻译"><a href="#一-、论文翻译" class="headerlink" title="一 、论文翻译"></a>一 、论文翻译</h1><blockquote>
<p>paper:<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener"> </a></p>
</blockquote>
<blockquote>
<p>论文解决的主要问题是深层的神经网络很难训练；提出了一种残差学习框架来减轻网络训练。</p>
</blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>更深的神经网络更难训练。我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。在ImageNet数据集上我们评估了深度高达152层的残差网络——比VGG深8倍但仍具有较低的复杂度。这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。这个结果在ILSVRC 2015分类任务上赢得了第一名。我们也在CIFAR-10上分析了100层和1000层的残差网络。</p>
<p>对于许多视觉识别任务而言，表示的深度是至关重要的。仅由于我们非常深度的表示，我们便在COCO目标检测数据集上得到了28%的相对提高。深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>深度卷积神经网络导致了图像分类的一系列突破。深度网络自然地将低/中/高级特征和分类器以端到端多层方式进行集成，特征的“级别”可以通过堆叠层的数量（深度）来丰富。最近的证据显示网络深度至关重要，在具有挑战性的ImageNet数据集上领先的结果都采用了“非常深”的模型，深度从16到30之间。许多其它重要的视觉识别任务也从非常深的模型中得到了极大受益。</p>
<p>在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸这个众所周知的问题，它从一开始就阻碍了收敛。然而，这个问题通过标准初始化和中间标准化层在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</p>
<p>当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差，正如中报告的那样，并且由我们的实验完全证实。图1显示了一个典型的例子。<br><img src="https://img-blog.csdnimg.cn/20210117163439557.png#pic_center" alt="在这里插入图片描述"></p>
<p>图1 20层和56层的“简单”网络在CIFAR-10上的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。ImageNet上的类似现象如图4所示。</p>
<p>退化（训练准确率）表明不是所有的系统都很容易优化。让我们考虑一个较浅的架构及其更深层次的对象，为其添加更多的层。存在通过构建得到更深层模型的解决方案：添加的层是恒等映射，其他层是从学习到的较浅模型的拷贝。 这种构造解决方案的存在表明，较深的模型不应该产生比其对应的较浅模型更高的训练误差。但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。</p>
<p>在本文中，我们通过引入深度残差学习框架解决了退化问题。我们明确地让这些层拟合残差映射，而不是希望每几个堆叠的层直接拟合期望的基础映射。形式上，将期望的基础映射表示为$H(x)$，我们将堆叠的非线性层拟合另一个映射$F(x):=H(x)−x$。原始的映射重写为$F(x)+x$。我们假设残差映射比原始的、未参考的映射更容易优化。在极端情况下，如果一个恒等映射是最优的，那么将残差置为零比通过一堆非线性层来拟合恒等映射更容易。</p>
<p>公式F(x)+x可以通过带有“快捷连接”的前向神经网络（图2）来实现。快捷连接是那些跳过一层或更多层的连接。在我们的案例中，快捷连接简单地执行恒等映射，并将其输出添加到堆叠层的输出（图2）。恒等快捷连接既不增加额外的参数也不增加计算复杂度。整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库轻松实现，而无需修改求解器。<br><img src="https://img-blog.csdnimg.cn/20210117163620107.png#pic_center" alt="在这里插入图片描述"></p>
<center>图2. 残差学习：构建块</center>
<br>

<p>我们在ImageNet上进行了综合实验来显示退化问题并评估我们的方法。我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</p>
<p>CIFAR-10数据集上也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。我们在这个数据集上展示了成功训练的超过100层的模型，并探索了超过1000层的模型。</p>
<p>在ImageNet分类数据集中，我们通过非常深的残差网络获得了很好的结果。我们的152层残差网络是ImageNet上最深的网络，同时还具有比VGG网络更低的复杂性。我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。极深的表示在其它识别任务中也有极好的泛化性能，并带领我们在进一步赢得了第一名：包括ILSVRC &amp; COCO 2015竞赛中的ImageNet检测，ImageNet定位，COCO检测和COCO分割。坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>残差表示。在图像识别中，VLAD是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量可以表示为VLAD的概率版本。它们都是图像检索和图像分类中强大的浅层表示。对于矢量量化，编码残差矢量被证明比编码原始矢量更有效。</p>
<p>在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。Multigrid的替代方法是层次化基础预处理，它依赖于表示两个尺度之间残差向量的变量。已经被证明这些求解器比不知道解的残差性质的标准求解器收敛得更快。这些方法表明，良好的重构或预处理可以简化优化。</p>
<p>快捷连接。导致快捷连接的实践和理论已经被研究了很长时间。训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出。一些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸。其它论文提出了通过快捷连接实现层间响应，梯度和传播误差的方法。在[44]中，一个“inception”层由一个快捷分支和一些更深的分支组成。</p>
<p>和我们同时进行的工作，“highway networks” 提出了门功能的快捷连接。这些门是数据相关且有参数的，与我们不具有参数的恒等快捷连接相反。当门控快捷连接“关闭”（接近零）时，高速网络中的层表示非残差函数。相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。此外，高速网络还没有证实极度增加的深度（例如，超过100个层）带来的准确性收益。</p>
<h2 id="3-深度残差学习"><a href="#3-深度残差学习" class="headerlink" title="3 深度残差学习"></a>3 深度残差学习</h2><h3 id="3-1-残差学习"><a href="#3-1-残差学习" class="headerlink" title="3.1 残差学习"></a>3.1 残差学习</h3><p>我们考虑H(x)作为几个堆叠层（不必是整个网络）要拟合的基础映射，x表示这些层中第一层的输入。假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即H(x)−x(假设输入输出是相同维度)。因此，我们明确让这些层近似参数函数 F(x):=H(x)−x，而不是期望堆叠层近似H(x)。因此原始函数变为F(x)+x。尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。</p>
<p>关于退化问题的反直觉现象激发了这种重构（图1左）。正如我们在引言中讨论的那样，如果添加的层可以被构建为恒等映射，更深模型的训练误差应该不大于它对应的更浅版本。退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</p>
<p>在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。<br><img src="https://img-blog.csdnimg.cn/20210117163809689.png#pic_center" alt="在这里插入图片描述"><br>图7。层响应在CIFAR-10上的标准差（std）。这些响应是每个3×3层的输出，在BN之后非线性之前。上面：以原始顺序显示层。下面：响应按降序排列。</p>
<h3 id="3-2-快捷恒等映射"><a href="#3-2-快捷恒等映射" class="headerlink" title="3.2 快捷恒等映射"></a>3.2 快捷恒等映射</h3><p>我们每隔几个堆叠层采用残差学习。构建块如图2所示。在本文中我们考虑构建块正式定义为：</p>
<p>$$y=F(x,W_i)+x \tag{1}$$<br>$x$和$y$是考虑的层的输入和输出向量。函数$F(x,W_i)$表示要学习的残差映射。图2中的例子有两层，$F=W_2\sigma(W_1x)$中$\sigma$表示ReLU，为了简化写法忽略偏置项。$F+x$操作通过快捷连接和各个元素相加来执行。在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。</p>
<p>方程(1)中的快捷连接既没有引入外部参数又没有增加计算复杂度。这不仅在实践中有吸引力，而且在简单网络和残差网络的比较中也很重要。我们可以公平地比较同时具有相同数量的参数，相同深度，宽度和计算成本的简单/残差网络（除了不可忽略的元素加法之外）。</p>
<p>方程(1)中$x$和$F$的维度必须是相等的。如果不是这种情况（例如，当更改输入/输出通道时），我们可以通过快捷连接执行线性投影Ws来匹配维度：</p>
<p>$$y=F(x,W_i)+W_sx \tag{2}$$<br>我们也可以使用方程(1)中的方阵Ws。但是我们将通过实验表明，恒等映射足以解决退化问题，并且是合算的，因此Ws仅在匹配维度时使用。</p>
<p>残差函数$F$的形式是可变的。本文中的实验包括有两层或三层（图5）的函数F，同时可能有更多的层。但如果$F$只有一层，方程(1)类似于线性层：$y=W_1x+x$，我们没有看到优势。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117163857859.png#pic_center" alt="在这里插入图片描述"><br>图5。ImageNet的深度残差函数F。左：ResNet-34的构建块（在56×56的特征图上），如图3。右：ResNet-50/101/152的“bottleneck”构建块。</p>
<p>我们还注意到，为了简单起见，尽管上述符号是关于全连接层的，但它们同样适用于卷积层。函数$F(x,W_i)$可以表示多个卷积层。元素加法在两个特征图上逐通道进行。</p>
<h3 id="3-3-网络架构"><a href="#3-3-网络架构" class="headerlink" title="3.3. 网络架构"></a>3.3. 网络架构</h3><p>我们测试了各种简单/残差网络，并观察到了一致的现象。为了提供讨论的实例，我们描述了ImageNet的两个模型如下。</p>
<p><strong>简单网络</strong>。 我们简单网络的基准（图3，中间）主要受到VGG网络（图3，左图）的哲学启发。卷积层主要有3×3的滤波器，并遵循两个简单的设计规则：（i）对于相同的输出特征图尺寸，层具有相同数量的滤波器；（ii）如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度。我们通过步长为2的卷积层直接执行下采样。网络以全局平均池化层和具有softmax的1000维全连接层结束。图3（中间）的加权层总数为34。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164024281.png#pic_center" alt="在这里插入图片描述"></p>
<p>图3。ImageNet的网络架构例子。左：作为参考的VGG-19模型40。中：具有34个参数层的简单网络（36亿FLOPs）。右：具有34个参数层的残差网络（36亿FLOPs）。带点的快捷连接增加了维度。表1显示了更多细节和其它变种。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164108856.png#pic_center" alt="在这里插入图片描述"><br>表1。ImageNet架构。构建块显示在括号中（也可看图5），以及构建块的堆叠数量。下采样通过步长为2的conv3_1, conv4_1和conv5_1执行。</p>
<p>值得注意的是我们的模型与VGG网络（图3左）相比，有更少的滤波器和更低的复杂度。我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</p>
<p><strong>残差网络</strong>。 基于上述的简单网络，我们插入快捷连接（图3，右），将网络转换为其对应的残差版本。当输入和输出具有相同的维度时（图3中的实线快捷连接）时，可以直接使用恒等快捷连接（方程（1））。当维度增加（图3中的虚线快捷连接）时，我们考虑两个选项：（A）快捷连接仍然执行恒等映射，额外填充零输入以增加维度。此选项不会引入额外的参数；（B）方程（2）中的投影快捷连接用于匹配维度（由1×1卷积完成）。对于这两个选项，当快捷连接跨越两种尺寸的特征图时，它们执行时步长为2。</p>
<h3 id="3-4-实现"><a href="#3-4-实现" class="headerlink" title="3.4 实现"></a>3.4 实现</h3><p>ImageNet中我们的实现遵循的实践。调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强。224×224裁剪是从图像或其水平翻转中随机采样，并逐像素减去均值。使用了标准颜色增强。在每个卷积之后和激活之前，我们采用批量归一化（BN）。我们按照[13]的方法初始化权重，从零开始训练所有的简单/残差网络。我们使用批大小为256的SGD方法。学习速度从0.1开始，当误差稳定时学习率除以10，并且模型训练高达$60 \times10^4$次迭代。我们使用的权重衰减为0.0001，动量为0.9。</p>
<p>在测试阶段，为了比较学习我们采用标准的10-crop测试。对于最好的结果，我们采用全卷积形式，并在多尺度上对分数进行平均（图像归一化，短边位于{224, 256, 384, 480, 640}中）。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><h3 id="4-1-ImageNet分类"><a href="#4-1-ImageNet分类" class="headerlink" title="4.1 ImageNet分类"></a>4.1 ImageNet分类</h3><p>我们在ImageNet 2012分类数据集对我们的方法进行了评估，该数据集由1000个类别组成。这些模型在128万张训练图像上进行训练，并在5万张验证图像上进行评估。我们也获得了测试服务器报告的在10万张测试图像上的最终结果。我们评估了top-1和top-5错误率。</p>
<p>简单网络。我们首先评估18层和34层的简单网络。34层简单网络在图3（中间）。18层简单网络是一种类似的形式。有关详细的体系结构，请参见表1。</p>
<p>表2中的结果表明，较深的34层简单网络比较浅的18层简单网络有更高的验证误差。为了揭示原因，在图4（左图）中，我们比较训练过程中的训练/验证误差。我们观察到退化问题——虽然18层简单网络的解空间是34层简单网络解空间的子空间，但34层简单网络在整个训练过程中具有较高的训练误差。</p>
<p><img src="https://img-blog.csdnimg.cn/2021011716423757.png#pic_center" alt="在这里插入图片描述"><br>表2。ImageNet验证集上的Top-1错误率(%，10个裁剪图像测试)。相比于对应的简单网络，ResNet没有额外的参数。图4显示了训练过程。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164339696.png#pic_center" alt="在这里插入图片描述"><br>图4。在ImageNet上训练。细曲线表示训练误差，粗曲线表示中心裁剪图像的验证误差。左：18层和34层的简单网络。右：18层和34层的ResNet。在本图中，残差网络与对应的简单网络相比没有额外的参数。</p>
<p>我们认为这种优化难度不可能是由于梯度消失引起的。这些简单网络使用BN训练，这保证了前向传播信号有非零方差。我们还验证了反向传播的梯度，结果显示其符合BN的正常标准。因此既不是前向信号消失也不是反向信号消失。实际上，34层简单网络仍能取得有竞争力的准确率（表3），这表明在某种程度上来说求解器仍工作。我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。这种优化困难的原因将来会研究。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164446577.png#pic_center" alt="在这里插入图片描述"><br>表3。ImageNet验证集错误率（%，10个裁剪图像测试）。VGG16是基于我们的测试结果的。ResNet-50/101/152的选择B仅使用投影增加维度。</p>
<p><strong>残差网络</strong>。接下来我们评估18层和34层残差网络（ResNets）。基准架构与上述的简单网络相同，如图3（右）所示，预计每对3×3滤波器都会添加快捷连接。在第一次比较（表2和图4右侧）中，我们对所有快捷连接都使用恒等映射和零填充以增加维度（选项A）。所以与对应的简单网络相比，它们没有额外的参数。</p>
<p>我们从表2和图4中可以看到三个主要的观察结果。首先，残留学习的情况变了——34层ResNet比18层ResNet更好（2.8％）。更重要的是，34层ResNet显示出较低的训练误差，并且可以泛化到验证数据。这表明在这种情况下，退化问题得到了很好的解决，我们从增加的深度中设法获得了准确性收益。</p>
<p>第二，与对应的简单网络相比，由于成功的减少了训练误差，34层ResNet降低了3.5%的top-1错误率。这种比较证实了在极深系统中残差学习的有效性。</p>
<p>最后，我们还注意到18层的简单/残差网络同样地准确（表2），但18层ResNet收敛更快（图4右和左）。当网络“不过度深”时（18层），目前的SGD求解器仍能在简单网络中找到好的解。在这种情况下，ResNet通过在早期提供更快的收敛简便了优化。</p>
<p>恒等和投影快捷连接我们已经表明没有参数，恒等快捷连接有助于训练。接下来我们调查投影快捷连接（方程2）。在表3中我们比较了三个选项：(A) 零填充快捷连接用来增加维度，所有的快捷连接是没有参数的（与表2和图4右相同）；(B)投影快捷连接用来增加维度，其它的快捷连接是恒等的；（C）所有的快捷连接都是投影。</p>
<p>表3显示，所有三个选项都比对应的简单网络好很多。选项B比A略好。我们认为这是因为A中的零填充确实没有残差学习。选项C比B稍好，我们把这归因于许多（十三）投影快捷连接引入了额外参数。但A/B/C之间的细微差异表明，投影快捷连接对于解决退化问题不是至关重要的。因为我们在本文的剩余部分不再使用选项C，以减少内存/时间复杂性和模型大小。恒等快捷连接对于不增加下面介绍的瓶颈结构的复杂性尤为重要。</p>
<p>更深的瓶颈结构。接下来我们描述ImageNet中我们使用的更深的网络网络。由于关注我们能承受的训练时间，我们将构建块修改为瓶颈设计。对于每个残差函数F，我们使用3层堆叠而不是2层（图5）。三层是1×1，3×3和1×1卷积，其中1×1层负责减小然后增加（恢复）维度，使3×3层成为具有较小输入/输出维度的瓶颈。图5展示了一个示例，两个设计具有相似的时间复杂度。</p>
<p>无参数恒等快捷连接对于瓶颈架构尤为重要。如果图5（右）中的恒等快捷连接被投影替换，则可以显示出时间复杂度和模型大小加倍，因为快捷连接是连接到两个高维端。因此，恒等快捷连接可以为瓶颈设计得到更有效的模型。</p>
<p>50层ResNet：我们用3层瓶颈块替换34层网络中的每一个2层块，得到了一个50层ResNet（表1）。我们使用选项B来增加维度。该模型有38亿FLOP。</p>
<p>101层和152层ResNet：我们通过使用更多的3层瓶颈块来构建101层和152层ResNets（表1）。值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</p>
<p>50/101/152层ResNet比34层ResNet的准确性要高得多（表3和4）。我们没有观察到退化问题，因此可以从显著增加的深度中获得显著的准确性收益。所有评估指标都能证明深度的收益（表3和表4）。</p>
<p>与最先进的方法比较。在表4中，我们与以前最好的单一模型结果进行比较。我们基准的34层ResNet已经取得了非常有竞争力的准确性。我们的152层ResNet具有单模型4.49％的top-5错误率。这种单一模型的结果胜过以前的所有综合结果（表5）。我们结合了六种不同深度的模型，形成一个集合（在提交时仅有两个152层）。这在测试集上得到了3.5％的top-5错误率（表5）。这次提交在2015年ILSVRC中荣获了第一名。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164730510.png#pic_center" alt="在这里插入图片描述"><br>表4。单一模型在ImageNet验证集上的错误率（%）(除了†是测试集上报告的错误率)。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164654928.png#pic_center" alt="在这里插入图片描述"></p>
<p>表5。模型综合的错误率(%)。top-5错误率是ImageNet测试集上的并由测试服务器报告的。</p>
<h3 id="4-2-CIFAR-10和分析"><a href="#4-2-CIFAR-10和分析" class="headerlink" title="4.2 CIFAR-10和分析"></a>4.2 CIFAR-10和分析</h3><p>我们对CIFAR-10数据集进行了更多的研究，其中包括10个类别中的5万张训练图像和1万张测试图像。我们介绍了在训练集上进行训练和在测试集上进行评估的实验。我们的焦点在于极深网络的行为，但不是推动最先进的结果，所以我们有意使用如下的简单架构。</p>
<p>简单/残差架构遵循图3（中/右）的形式。网络输入是32×32的图像，每个像素减去均值。第一层是3×3卷积。然后我们在大小为{32,16,8}的特征图上分别使用了带有3×3卷积的6n个堆叠层，每个特征图大小使用2n层。滤波器数量分别为{16,32,64}。下采样由步长为2的卷积进行。网络以全局平均池化，一个10维全连接层和softmax作为结束。共有6n+2个堆叠的加权层。下表总结了这个架构：</p>
<p><img src="https://img-blog.csdnimg.cn/20201218141750496.png#pic_center" alt="在这里插入图片描述"><br>当使用快捷连接时，它们连接到成对的3×3卷积层上（共3n个快捷连接）。在这个数据集上，我们在所有案例中都使用恒等快捷连接（即选项A），因此我们的残差模型与对应的简单模型具有完全相同的深度，宽度和参数数量。</p>
<p>我们使用的权重衰减为0.0001和动量为0.9，并采用和BN中的权重初始化，但没有使用丢弃。这些模型在两个GPU上进行训练，批处理大小为128。我们开始使用的学习率为0.1，在32k次和48k次迭代后学习率除以10，并在64k次迭代后终止训练，这是由45k/5k的训练/验证集分割决定的。我们按照简单数据增强进行训练：每边填充4个像素，并从填充图像或其水平翻转图像中随机采样32×32的裁剪图像。对于测试，我们只评估原始32×32图像的单一视图。</p>
<p>我们比较了n=3,5,7,9，得到了20层，32层，44层和56层的网络。图6（左）显示了简单网络的行为。深度简单网络经历了深度增加，随着深度增加表现出了更高的训练误差。这种现象类似于ImageNet中（图4，左）和MNIST中的现象，表明这种优化困难是一个基本的问题。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117164911125.png#pic_center" alt="在这里插入图片描述"><br>图6。在CIFAR-10上训练。虚线表示训练误差，粗线表示测试误差。左：简单网络。简单的110层网络错误率超过60%没有展示。中间：ResNet。右：110层ResNet和1202层ResNet。</p>
<p>图6（中）显示了ResNet的行为。与ImageNet的情况类似（图4，右），我们的ResNet设法克服优化困难并随着深度的增加展示了准确性收益。</p>
<p>我们进一步探索了n=18得到了110层的ResNet。在这种情况下，我们发现0.1的初始学习率对于收敛来说太大了。因此我们使用0.01的学习率开始训练，直到训练误差低于80%（大约400次迭代），然后学习率变回到0.1并继续训练。学习过程的剩余部分与前面做的一样。这个110层网络收敛的很好（图6，中）。它与其它的深且窄的网络例如FitNet和Highway相比有更少的参数，但结果仍在目前最好的结果之间（6.43%，表6）。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117165039510.png#pic_center" alt="在这里插入图片描述"><br>表6。在CIFAR-10测试集上的分类误差。所有的方法都使用了数据增强。对于ResNet-110，我们运行了5次并展示了“最好的(mean±std)”</p>
<p><strong>层响应分析</strong>。图7显示了层响应的标准偏差（std）。这些响应每个3×3层的输出，在BN之后和其他非线性（ReLU/加法）之前。对于ResNets，该分析揭示了残差函数的响应强度。图7显示ResNet的响应比其对应的简单网络的响应更小。这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。我们还注意到，更深的ResNet具有较小的响应幅度，如图7中ResNet-20，56和110之间的比较所证明的。当层数更多时，单层ResNet趋向于更少地修改信号。</p>
<p><strong>探索超过1000层</strong>。我们探索超过1000层的过深的模型。我们设置n=200，得到了1202层的网络，其训练如上所述。我们的方法显示没有优化困难，这个103层网络能够实现训练误差&lt;0.1％（图6，右图）。其测试误差仍然很好（7.93％，表6）。</p>
<p>但是，这种极深的模型仍然存在着开放的问题。这个1202层网络的测试结果比我们的110层网络的测试结果更差，虽然两者都具有类似的训练误差。我们认为这是因为过拟合。对于这种小型数据集，1202层网络可能是不必要的大（19.4M）。在这个数据集应用强大的正则化，如maxout或者dropout来获得最佳结果。在本文中，我们不使用maxout/dropout，只是简单地通过设计深且窄的架构简单地进行正则化，而不会分散集中在优化难点上的注意力。但结合更强的正规化可能会改善结果，我们将来会研究。</p>
<h3 id="4-3-在PASCAL和MS-COCO上的目标检测"><a href="#4-3-在PASCAL和MS-COCO上的目标检测" class="headerlink" title="4.3 在PASCAL和MS COCO上的目标检测"></a>4.3 在PASCAL和MS COCO上的目标检测</h3><p>我们的方法对其他识别任务有很好的泛化性能。表7和表8显示了PASCAL VOC 2007和2012以及COCO的目标检测基准结果。我们采用更快的R-CNN作为检测方法。在这里，我们感兴趣的是用ResNet-101替换VGG-16。使用这两种模式的检测实现（见附录）是一样的，所以收益只能归因于更好的网络。最显著的是，在有挑战性的COCO数据集中，COCO的标准度量指标（mAP@[.5，.95]）增长了6.0％，相对改善了28％。这种收益完全是由于学习表示。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117165203540.png#pic_center" alt="在这里插入图片描述"><br>表7。在PASCAL VOC 2007/2012测试集上使用基准Faster R-CNN的目标检测mAP(%)。更好的结果请看附录。</p>
<p><img src="https://img-blog.csdnimg.cn/20210117165237461.png#pic_center" alt="在这里插入图片描述"><br>表8。在COCO验证集上使用基准Faster R-CNN的目标检测mAP(%)。更好的结果请看附录。</p>
<p>基于深度残差网络，我们在ILSVRC &amp; COCO 2015竞赛的几个任务中获得了第一名，分别是：ImageNet检测，ImageNet定位，COCO检测，COCO分割。跟多细节请看附录。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="A-对象检测基准"><a href="#A-对象检测基准" class="headerlink" title="A.对象检测基准"></a>A.对象检测基准</h3><p>在本节中，我们介绍基于基线Faster R-CNN 系统的检测方法。这些模型由ImageNet分类模型初始化，然后对目标检测数据进行微调。在ILSVRC和COCO 2015检测竞赛时，我们已经使用ResNet-50 / 101进行了实验。</p>
<p>与以往使用的VGG-16不同，我们的ResNet没有隐藏fc层。我们采用“转换功能地图上的网络”（NoC）的想法来解决此问题。我们使用那些步幅不大于16个像素的图层（即conv1，conv2x，conv3x和conv4x，在ResNet-101中总共91个convlayers;表1）来计算全图像共享conv特征图。我们认为这些层类似于VGG-16中的13个conv层，这样做，ResNet和VGG-16都具有相同总步幅（16像素）的conv特征图。这些层由区域提议网络（RPN，生成300个提议）和快速R-CNN检测网络共享。 RoI合并在conv51之前执行。在此RoI共享的功能上，每个区域都采用了conv5x和up的所有层，起到了VGG-16的fc层的作用。最终分类层被两个同级层替代（分类和框回归）。</p>
<p>对于BN层的使用，在预训练之后，我们在ImageNet上计算每个层的BN统计量（均值和方差）。训练集。然后在微调期间固定BN层以进行对象检测。这样，BNlayers成为具有恒定偏移和比例的线性激活，并且不会通过微调更新BN统计信息。固定BN层主要是为了减少快速R-CNN训练中的内存消耗。</p>
<h4 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><p>对于PASCAL VOC 2007测试集，我们使用VOC 2007中的5ktrainvalimage和VOC 2012中的16ktrain-valimage进行训练（“ 07 + 12”）。对于PASCAL VOC 2012测试集，我们使用VOC 2007中的10ktrainval + testimages和VOC 2012中的16ktrainvalimages进行训练（“ 07 ++ 12”）。用于训练Faster R-CNN的超参数与[32]中的相同。表7示出了结果。与VGG-16相比，ResNet-101将mAP提高了3％以上。收益的增加完全是由于ResNet获得了改进的功能。</p>
<h4 id="MS-COCO"><a href="#MS-COCO" class="headerlink" title="MS COCO"></a>MS COCO</h4><p>MS COCO数据集涉及80个对象类别。我们评估了PASCAL VOC指标（mAP @IoU = 0.5）和标准COCO指标（mAP @IoU = .5：.05：.95）。我们将火车上的80k图像用于训练，将val上的40k图像用于评估。我们的COCO检测系统与PASCALVOC相似。我们使用8 GPU实现训练COCO模型，因此RPN步骤的最小批量大小为8张图像（即每个GPU 1个），而快速R-CNN步骤的最小批量大小为16张图像。 RPN步骤和Fast R-CNN步骤均以0.001的学习率进行了240k迭代的训练，然后以0.0001进行了80k迭代的训练。</p>
<p>表8显示了MS COCO验证集的结果。 ResNet-101的mAP@[.5，.95]比VGG-16增加了6％，相对改善了28％，这完全是由更好的网络所学到的功能所致。值得注意的是，mAP@[.5，.95]的绝对增长（6.0％）几乎与mAP @ .5的绝对增长（6.9％）一样大。这表明，加深者网络可以同时提高识别能力和定位能力。</p>
<h3 id="B-对象检测的改进"><a href="#B-对象检测的改进" class="headerlink" title="B.对象检测的改进"></a>B.对象检测的改进</h3><p>为了完整起见，我们报告了比赛的进步。这些改进基于深层功能，因此应该从残余学习中受益。</p>
<h4 id="MS-COCO-1"><a href="#MS-COCO-1" class="headerlink" title="MS COCO"></a>MS COCO</h4><p><em>Box细化</em>。我们的盒细化部分遵循迭代定位。在Faster R-CNN中，最终输出是与其提案框不同的回归框。因此，我们可以从回归框中合并一个新功能，并获得新的分类分数和新的回归框。我们将这300个新预测与原始300个预测结合在一起。非最大抑制（NMS）应用于预测框的并集，使用IoUthreshold为0.3 ，然后进行框投票。盒子重新细化可将mAP提高约2点（表9）</p>
<p><img src="https://img-blog.csdnimg.cn/20210119103110772.png#pic_center" alt="在这里插入图片描述"><br>表9。使用Faster R-CNN和ResNet-101在MS COCO上进行的对象检测改进。</p>
<p><em>全局上下文</em>。我们在FastR-CNN步骤中结合全局上下文。给定全图像转换特征图，通过全局空间金字塔池（具有“单级”金字塔）来合并特征，可以使用整个图像的边界框作为RoI将其实现为“ RoI”池。将此合并的功能馈入后RoI层以获得全局上下文功能。将此全局特征与原始的按区域特征串联在一起，然后是同级分类和框回归图层。这种新的结构是端到端的。全局上下文将mAP@ .5提高了大约1个点（表9）</p>
<p><em>多尺度测试</em>。在上面，所有结果都是通过单尺度训练/测试获得的，其中图像的较短边是600像素。通过从特征金字塔中选择尺度来开发多尺度训练/测试，使用maxout层来进行了开发。在当前的实现中，我们已经执行了以下的多尺度测试；由于时间有限，我们尚未执行多尺度训练。此外，我们仅针对Fast R-CNN步骤（但尚未针对RPN步骤）执行多尺度测试。通过训练有素的模型，我们可以在图像金字塔上计算转换特征图，其中图像的短边为s∈{200,400,600,800,1000}</p>
<p>我们从下面的金字塔中选择两个相邻的比例尺。 RoI池和后续层在这两个比例尺的特征图上执行，它们被maxout合并。多尺度测试将mAP提高了2个百分点（表9）。</p>
<p><em>使用验证数据</em>。接下来，我们使用80k + 40k trainval集合进行训练，使用20k test-dev集合进行评估。测试开发集没有公开可用的基础事实，并且结果由评估服务器报告。在此设置下，结果的mAP@ .5为55.7％，mAP@ [.5，.95]为34.9％（表9）。这是我们的单模结果。</p>
<p><em>集成</em>。在Faster R-CNN中，该系统旨在学习区域提议以及对象分类器，因此可以使用集成来完成这两项任务。我们使用一个整体来提议区域，并且提议的联合集由每个区域分类器的整体来处理。表9显示了基于3个网络的合计结果。在测试开发集上，mAP分别为59.0％和37.4％，该结果在COCO 2015的检测任务中排名第一。</p>
<h4 id="PASCAL-VOC-1"><a href="#PASCAL-VOC-1" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><p>我们基于上述模型重新访问PASCAL VOC数据集。使用COCO数据集上的单个模型（表9中为55.7％mAP@ .5），我们可以在PAS-CAL VOC集合上微调该模型。还采用了改进的框精，上下文和多尺度测试。通过这样做我们在PASCAL VOC 2007（表10）和83.8％PASCAL VOC 2012（表11）上实现了85.6％的mAP。 PASCAL VOC 2012的结果比以前的最新结果高10点。</p>
<p><img src="https://img-blog.csdnimg.cn/20210119103800709.png#pic_center" alt="在这里插入图片描述"><br>表10。 PASCAL VOC 2007测试集的检测结果。基线是Faster R-CNN系统。 “ baseline +++”系统在表9中包括框优化，上下文和多尺度测试。</p>
<p><img src="https://img-blog.csdnimg.cn/20210119104141236.png#pic_center" alt="在这里插入图片描述"><br>表11. PASCAL VOC 2012测试集的检测结果。基线是Faster R-CNN系统。表9中的“基线+++”系统包括框优化，上下文和多尺度测试。</p>
<h4 id="ImageNet检测"><a href="#ImageNet检测" class="headerlink" title="ImageNet检测"></a>ImageNet检测</h4><p>ImageNet检测（DET）任务涉及200个对象类别。精度由mAP@ .5评估。 ImageNet DET的目标检测算法与表9中的MS COCO相同。网络在1000类ImageNet分类集上进行了预训练，并在DET数据上进行了微调。我们将验证设置分为两部分（val1/val2）。我们使用DET训练集和val1set对检测模型进行微调。 val2集用于验证。我们不使用其他ILSVRC 2015数据。我们使用ResNet-101的单一模型具有58.8% mAP且 3个模型的集成在DET测试集上具有62.1％的mAP（表12）。<em>该结果在ILSVRC 2015的ImageNet检测任务中获得了第一名</em>，比第二名多了8.5分（绝对值）。</p>
<h3 id="C-ImageNet定位"><a href="#C-ImageNet定位" class="headerlink" title="C. ImageNet定位"></a>C. ImageNet定位</h3><p>ImageNet定位（LOC）任务需要对对象进行分类和定位对象。 我们假设首先采用图像级分类器来预测图像的类别标签，而定位算法仅考虑了基于预测类别的边界框预测。我们采用“每类回归”（PCR）策略，为每个类学习边框回归。我们预先训练网络以进行ImageNet分类，然后对其进行微调以进行定位。我们在提供的1000级ImageNet训练集中训练网络。</p>
<p>我们的定位算法基于RPN框架，并做了一些修改。与不区分类别的方式不同，我们用于定位的RPN是按aper-classform设计的。该RPN以两个同级1×1卷积层结束，用于二进制分类（cls）和边框回归（reg），cls和reg 都属于per类。具体来说，cls具有1000维的输出，并且每个维都是二进制逻辑回归，用于预测是否为对象类。 reg具有1000×4-d输出，由1000个类别的盒形回归组成。我们的边界框回归是参考每个位置的多个翻译不变的“锚定”框。</p>
<p>与我们的ImageNet分类训练（第3.4节）中一样，我们随机采样224×224种作物以进行数据增强。我们使用256幅图像的小批量进行微调。为了避免负样本占主导地位，每个图像随机采样8个锚点，其中正负锚点的比例为1：1 。为了进行测试，将网络完全卷积地应用到图像上。</p>
<p>表13比较了定位的结果。我们首先使用地面真理类作为分类预测来执行“ oracle”测试。VGG的论文使用groundtruth类的中心裁剪误差为33.1％（表13）。在相同的设置下，我们使用ResNet-101 net的RPN方法可将中央作物误差大大降低至13.3％。此比较证明了我们框架的出色性能。通过密集（完全卷积）和多尺度测试，我们的ResNet-101使用地面真理类的错误率为11.7％。使用ResNet-101预测类别（前5名分类错误率为4.6％，表4），前5名定位错误率为14.4％。</p>
<p><img src="https://img-blog.csdnimg.cn/20210119105634292.png#pic_center" alt="在这里插入图片描述"><br>表13. ImageNet验证上的定位错误率（％）。在“ GT类的LOC错误” 列中，使用了地面真实类。在“测试”列中，“ 1-crop”表示对224×224像素的中心作物进行测试，“ dense”表示密集（完全卷积）和多尺度测试。</p>
<p>以上结果仅基于Faster R-CNN 中的投标网络（RPN）。可以使用Faster R-CNN中的检测网络（Fast R-CNN）来改善结果。但是我们注意到，在该数据集上，一张图像通常包含一个主要对象，并且投标区域彼此高度重叠，因此具有非常相似的RoI合并特征。结果，Fast R-CNN 的以图像为中心的训练会产生小的变化样本，这对于随机训练可能是不希望的。因此，在我们目前的实验中，我们使用以RoI为中心的原始R-CNN代替Fast R-CNN。</p>
<p>我们的R-CNN实现如下。我们在训练图像上应用如上训练的每类RPN，以预测地面真理类的边界框。这些预测的框起着与类相关的建议的作用。对于每个训练图像，最高得分的200个建议被提取为训练R-CNN分类器的训练样本。图像区域是从提案中裁剪出来的，扭曲为224×224像素，然后馈入R-CNN中的分类网络。该网络的输出包括cls 和 reg的两个同级fc层，也为每类形式。此R-CNN网络在训练集上进行了微调，以RoI为中心的微型批次大小为256。进行测试时，RPN为每个预测的类别生成得分最高的200个提案，并且R-CNN网络用于更新这些提案的得分和方框位置。</p>
<p>这种方法将前5位的定位误差降低到10.6％（表13）。这是我们在验证集上的单模型结果。将网络集成用于分类和定位，我们在测试集上实现了9.0％的前五名本地化错误。这个数字大大超过了ILSVRC 14的结果（表14），显示出64％的错误相对减少。该结果在ILSVRC 2015的ImageNet本地化任务中获得了第一名。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="戈孔明 WeChat Pay">
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="戈孔明 Alipay">
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ResNetV1/" rel="tag"># ResNetV1</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/p/19637.html" rel="next" title="【图像分类—GoogLeNet Inception V3】Rethinking the Inception Architecture for Computer Vision">
                <i class="fa fa-chevron-left"></i> 【图像分类—GoogLeNet Inception V3】Rethinking the Inception Architecture for Computer Vision
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="戈孔明">
            
              <p class="site-author-name" itemprop="name">戈孔明</p>
              <p class="site-description motion-element" itemprop="description">湖南师范大学 | 计算机视觉</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/gkm0120" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:gkm0120@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons">
              </a>
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://gkm0120.cn/" title="孔明の博客" target="_blank">孔明の博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://hpcsiplab.hunnu.edu.cn/" title="LCSM" target="_blank">LCSM</a>
                  </li>
                
              </ul>
            </div>
          

          
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一-、论文翻译"><span class="nav-text">一 、论文翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-介绍"><span class="nav-text">1 介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-相关工作"><span class="nav-text">2 相关工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-深度残差学习"><span class="nav-text">3 深度残差学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-残差学习"><span class="nav-text">3.1 残差学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-快捷恒等映射"><span class="nav-text">3.2 快捷恒等映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-网络架构"><span class="nav-text">3.3. 网络架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-实现"><span class="nav-text">3.4 实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-实验"><span class="nav-text">4 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-ImageNet分类"><span class="nav-text">4.1 ImageNet分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-CIFAR-10和分析"><span class="nav-text">4.2 CIFAR-10和分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-在PASCAL和MS-COCO上的目标检测"><span class="nav-text">4.3 在PASCAL和MS COCO上的目标检测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-text">References</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#附录"><span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-对象检测基准"><span class="nav-text">A.对象检测基准</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PASCAL-VOC"><span class="nav-text">PASCAL VOC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MS-COCO"><span class="nav-text">MS COCO</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-对象检测的改进"><span class="nav-text">B.对象检测的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MS-COCO-1"><span class="nav-text">MS COCO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PASCAL-VOC-1"><span class="nav-text">PASCAL VOC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ImageNet检测"><span class="nav-text">ImageNet检测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-ImageNet定位"><span class="nav-text">C. ImageNet定位</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2020 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">戈孔明</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">114.1k</span>
  
</div>









<span id="runtime_span"></span>
<script type="text/javascript">function show_runtime(){window.setTimeout("show_runtime()",1000);X=new 
Date("3/15/2020 00:00:00");
Y=new Date();T=(Y.getTime()-X.getTime());M=24*60*60*1000;
a=T/M;A=Math.floor(a);b=(a-A)*24;B=Math.floor(b);c=(b-B)*60;C=Math.floor((b-B)*60);D=Math.floor((c-C)*60);
runtime_span.innerHTML="本站勉强运行: "+A+"天"+B+"小时"+C+"分"+D+"秒"}show_runtime();</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: true,
        appId: 'EJkE1pAGJj5cqmMY6l6NpT99-gzGzoHsz',
        appKey: 'RTVAilcFrOp07kcypuXbY3cG',
        placeholder: '留下你的足迹呗',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 代码块复制功能 -->
<script type="text/javascript" src="/js/src/clipboard.min.js"></script>
<script type="text/javascript" src="/js/src/clipboard-use.js"></script>
